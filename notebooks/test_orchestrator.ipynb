{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d7b8bf12",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Configuration Summary:\n",
            "----------------------\n",
            "LLM Provider: surf (Custom OpenAI-compatible endpoint (e.g., vLLM, TGI))\n",
            "LLM Model: Qwen 2.5 Coder 32B Instruct AWQ\n",
            "Planning Temperature: 0.0\n",
            "Player Temperature: 0.3\n",
            "Default Topology: default\n",
            "Default Metadata Standard: basic\n",
            "API Key (SURF_API_KEY): Set\n",
            "\n",
            "✅ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Metadata Agent - Orchestrator Test Notebook\n",
        "# =============================================================================\n",
        "# This notebook demonstrates the multi-agent metadata extraction system.\n",
        "#\n",
        "# Features:\n",
        "# - Plan Generation: LLM generates step-by-step extraction plan\n",
        "# - Parallel Execution: Multiple players work on each step\n",
        "# - Debate: Players critique and revise each other's work\n",
        "# - Synthesis: Results are consolidated into final output\n",
        "# =============================================================================\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from pprint import pprint\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Check config key\n",
        "from src.config import get_config_summary\n",
        "print(get_config_summary())\n",
        "\n",
        "# Import orchestrator components\n",
        "from src.orchestrator import Orchestrator\n",
        "from src.orchestrator.schemas import Plan, PlanStep\n",
        "from src.standards import METADATA_STANDARDS\n",
        "from src.topology import EXECUTION_TOPOLOGIES\n",
        "from src.players import PLAYER_CONFIGS\n",
        "\n",
        "print(\"✅ All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f988a5a",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "Set up the parameters for plan generation and execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86685c9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Configuration\n",
            "==================================================\n",
            "Dataset Path: ../data/biota.csv\n",
            "File Type: CSV\n",
            "Metadata Standard: basic\n",
            "Topology: fast\n",
            "\n",
            "Topology Details:\n",
            "  - Players per step: 2\n",
            "  - Debate rounds: 1\n",
            "  - Player pool: ['data_analyst', 'schema_expert']\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "DATASET_PATH = \"../data/biota.csv\"\n",
        "FILE_TYPE = \"CSV\"\n",
        "METADATA_STANDARD_NAME = \"basic\"  # Options: 'basic', 'dublin_core'\n",
        "TOPOLOGY_NAME = \"fast\"  # Options: 'default', 'fast', 'thorough', 'single'\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Configuration\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Dataset Path: {DATASET_PATH}\")\n",
        "print(f\"File Type: {FILE_TYPE}\")\n",
        "print(f\"Metadata Standard: {METADATA_STANDARD_NAME}\")\n",
        "print(f\"Topology: {TOPOLOGY_NAME}\")\n",
        "\n",
        "# Show topology details\n",
        "topology = EXECUTION_TOPOLOGIES[TOPOLOGY_NAME]\n",
        "print(f\"\\nTopology Details:\")\n",
        "print(f\"  - Players per step: {topology['players_per_step']}\")\n",
        "print(f\"  - Debate rounds: {topology['debate_rounds']}\")\n",
        "print(f\"  - Player pool: {topology['player_pool']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2056e3ca",
      "metadata": {},
      "source": [
        "## 2. Explore Available Configurations\n",
        "\n",
        "View available topologies and player configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f7628c30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Execution Topologies:\n",
            "==================================================\n",
            "\n",
            "default:\n",
            "  Description: Standard execution with 3 parallel players per step, 2 debate rounds, and comprehensive player pool.\n",
            "  Players/step: 3\n",
            "  Debate rounds: 2\n",
            "\n",
            "fast:\n",
            "  Description: Quick execution with 2 parallel players and minimal debate.\n",
            "  Players/step: 2\n",
            "  Debate rounds: 1\n",
            "\n",
            "thorough:\n",
            "  Description: Thorough execution with more players and extended debate.\n",
            "  Players/step: 4\n",
            "  Debate rounds: 3\n",
            "\n",
            "single:\n",
            "  Description: Single player execution with no debate. Fastest but least robust.\n",
            "  Players/step: 1\n",
            "  Debate rounds: 0\n"
          ]
        }
      ],
      "source": [
        "# Show all available topologies\n",
        "print(\"Available Execution Topologies:\")\n",
        "print(\"=\" * 50)\n",
        "for name, config in EXECUTION_TOPOLOGIES.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Description: {config['description']}\")\n",
        "    print(f\"  Players/step: {config['players_per_step']}\")\n",
        "    print(f\"  Debate rounds: {config['debate_rounds']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "41b276f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Player Roles:\n",
            "==================================================\n",
            "\n",
            "data_analyst:\n",
            "  Role: You are an expert data analyst. Your job is to perform statistical analysis on d...\n",
            "  Tools: ['get_file_info', 'get_row_count', 'get_column_names', 'get_column_statistics', 'get_missing_values']\n",
            "\n",
            "schema_expert:\n",
            "  Role: You are a database schema expert. Your job is to describe the structure of datas...\n",
            "  Tools: ['get_column_names', 'get_data_types', 'get_sample_rows']\n",
            "\n",
            "metadata_specialist:\n",
            "  Role: You are a metadata specialist familiar with standards like Dublin Core, DCAT, an...\n",
            "  Tools: None\n",
            "\n",
            "critic:\n",
            "  Role: You are a meticulous quality assurance critic. Your job is to review analyses fr...\n",
            "  Tools: None\n",
            "\n",
            "synthesizer:\n",
            "  Role: You are a metadata synthesizer. Your job is to consolidate multiple analyses int...\n",
            "  Tools: None\n"
          ]
        }
      ],
      "source": [
        "# Show all available player roles\n",
        "print(\"Available Player Roles:\")\n",
        "print(\"=\" * 50)\n",
        "for name, config in PLAYER_CONFIGS.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Role: {config['role_prompt'][:80]}...\")\n",
        "    tools = [t.name for t in config.get('tools', [])]\n",
        "    print(f\"  Tools: {tools if tools else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6596ae26",
      "metadata": {},
      "source": [
        "## 3. Plan Generation\n",
        "\n",
        "Generate a step-by-step plan for metadata extraction using the Orchestrator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a04fe194",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-15 14:15:08,917 - INFO - PlanExecutor initialized with topology: fast\n",
            "2026-01-15 14:15:08,918 - INFO -   Players per step: 2\n",
            "2026-01-15 14:15:08,919 - INFO -   Debate rounds: 1\n",
            "2026-01-15 14:15:08,920 - INFO -   Player pool: ['data_analyst', 'schema_expert']\n",
            "2026-01-15 14:15:08,920 - INFO - Orchestrator initialized with topology: fast\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata Standard:\n",
            "\n",
            "{\n",
            "    \"title\": \"...\",\n",
            "    \"description\": \"...\",\n",
            "    \"schema\": {\n",
            "        \"fields\": [\n",
            "            {\n",
            "                \"name\": \"...\",\n",
            "                \"type\": \"...\",\n",
            "                \"description\": \"...\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize the orchestrator\n",
        "orchestrator = Orchestrator(topology_name=TOPOLOGY_NAME)\n",
        "\n",
        "# Get the metadata standard content\n",
        "metadata_standard = METADATA_STANDARDS[METADATA_STANDARD_NAME]\n",
        "print(\"Metadata Standard:\")\n",
        "print(metadata_standard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6bd5080a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-15 14:15:12,875 - INFO - ============================================================\n",
            "2026-01-15 14:15:12,876 - INFO - GENERATING PLAN\n",
            "2026-01-15 14:15:12,877 - INFO - File type: CSV\n",
            "2026-01-15 14:15:12,877 - INFO - ============================================================\n",
            "2026-01-15 14:15:12,878 - INFO - Available players manifest:\n",
            "2026-01-15 14:15:12,878 - INFO - Player: data_analyst\n",
            "  Description: You are an expert data analyst. Your job is to perform statistical analysis on datasets, identify patterns, and extract meaningful insights. Focus on numerical summaries, distributions, and data quality.\n",
            "  Tools:\n",
            "    - get_file_info: Returns basic file information including size and estimated row count.\n",
            "Useful as a first step to understand the dataset scale.\n",
            "The input must be a valid file path.\n",
            "    - get_row_count: Returns the total number of rows in a structured data file (e.g., CSV).\n",
            "This is useful for getting a basic sense of the dataset's size.\n",
            "Optimized: counts rows without loading entire file into memory.\n",
            "The input must be a valid file path.\n",
            "    - get_column_names: Returns a list of column names from a structured data file (e.g., CSV).\n",
            "This is the first step to understanding the schema of the dataset.\n",
            "Optimized: reads only the header row.\n",
            "The input must be a valid file path.\n",
            "    - get_column_statistics: Returns basic statistics for all columns in a structured data file.\n",
            "For numeric columns: count, mean, std, min, max, quartiles.\n",
            "For non-numeric columns: count, unique, top, freq.\n",
            "Optimized: uses chunked processing for large files.\n",
            "The input must be a valid file path.\n",
            "    - get_missing_values: Returns a dictionary mapping column names to their count of missing values.\n",
            "Useful for data quality assessment.\n",
            "Optimized: uses chunked processing for large files.\n",
            "The input must be a valid file path.\n",
            "\n",
            "Player: schema_expert\n",
            "  Description: You are a database schema expert. Your job is to describe the structure of datasets, including column names, data types, relationships between fields, and recommend appropriate metadata schemas.\n",
            "  Tools:\n",
            "    - get_column_names: Returns a list of column names from a structured data file (e.g., CSV).\n",
            "This is the first step to understanding the schema of the dataset.\n",
            "Optimized: reads only the header row.\n",
            "The input must be a valid file path.\n",
            "    - get_data_types: Returns a dictionary mapping column names to their data types.\n",
            "Useful for understanding the schema and structure of the dataset.\n",
            "Optimized: infers types from a sample of rows for large files.\n",
            "The input must be a valid file path.\n",
            "    - get_sample_rows: Returns a string representation of the first n rows of the dataset.\n",
            "Useful for understanding the actual data content.\n",
            "Optimized: reads only the required rows.\n",
            "The input must be a valid file path.\n",
            "2026-01-15 14:15:12,879 - INFO - ----------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating plan...\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-15 14:15:33,156 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:15:33,355 - INFO - Plan generated successfully!\n",
            "2026-01-15 14:15:33,356 - INFO - Number of steps: 7\n",
            "2026-01-15 14:15:33,357 - INFO -   Step 1: get_file_info (player: data_analyst)\n",
            "2026-01-15 14:15:33,359 - INFO -   Step 2: get_row_count (player: data_analyst)\n",
            "2026-01-15 14:15:33,359 - INFO -   Step 3: get_column_names (player: schema_expert)\n",
            "2026-01-15 14:15:33,360 - INFO -   Step 4: get_data_types (player: schema_expert)\n",
            "2026-01-15 14:15:33,360 - INFO -   Step 5: get_column_statistics (player: data_analyst)\n",
            "2026-01-15 14:15:33,361 - INFO -   Step 6: get_missing_values (player: data_analyst)\n",
            "2026-01-15 14:15:33,361 - INFO -   Step 7: format_final_metadata (player: metadata_specialist)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Plan generated successfully!\n"
          ]
        }
      ],
      "source": [
        "# Generate the plan\n",
        "print(\"Generating plan...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "plan = orchestrator.generate_plan(\n",
        "    file_type=FILE_TYPE,\n",
        "    metadata_standard=metadata_standard\n",
        ")\n",
        "\n",
        "if plan:\n",
        "    print(\"\\n✅ Plan generated successfully!\")\n",
        "else:\n",
        "    print(\"\\n❌ Plan generation failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "885ee789",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Plan:\n",
            "==================================================\n",
            "\n",
            "Step 1: get_file_info\n",
            "  Player: data_analyst\n",
            "  Rationale: First, gather basic file information such as size and estimated row count to understand the dataset scale.\n",
            "  Inputs: {}\n",
            "  Outputs: ['file_info']\n",
            "\n",
            "Step 2: get_row_count\n",
            "  Player: data_analyst\n",
            "  Rationale: Get the exact number of rows in the dataset to ensure we have a precise count for our metadata.\n",
            "  Inputs: {'file_path': 'file_info'}\n",
            "  Outputs: ['row_count']\n",
            "\n",
            "Step 3: get_column_names\n",
            "  Player: schema_expert\n",
            "  Rationale: Identify the column names to understand the schema of the dataset.\n",
            "  Inputs: {'file_path': 'file_info'}\n",
            "  Outputs: ['column_names']\n",
            "\n",
            "Step 4: get_data_types\n",
            "  Player: schema_expert\n",
            "  Rationale: Determine the data types of each column to provide accurate schema information.\n",
            "  Inputs: {'file_path': 'file_info'}\n",
            "  Outputs: ['data_types']\n",
            "\n",
            "Step 5: get_column_statistics\n",
            "  Player: data_analyst\n",
            "  Rationale: Extract basic statistics for all columns to provide detailed insights into the dataset's numerical summaries and distributions.\n",
            "  Inputs: {'file_path': 'file_info'}\n",
            "  Outputs: ['column_statistics']\n",
            "\n",
            "Step 6: get_missing_values\n",
            "  Player: data_analyst\n",
            "  Rationale: Assess the quality of the dataset by identifying the count of missing values in each column.\n",
            "  Inputs: {'file_path': 'file_info'}\n",
            "  Outputs: ['missing_values']\n",
            "\n",
            "Step 7: format_final_metadata\n",
            "  Player: metadata_specialist\n",
            "  Rationale: Combine all extracted information into the required metadata standard format.\n",
            "  Inputs: {'file_info': 'file_info', 'row_count': 'row_count', 'column_names': 'column_names', 'data_types': 'data_types', 'column_statistics': 'column_statistics', 'missing_values': 'missing_values'}\n",
            "  Outputs: ['final_metadata']\n"
          ]
        }
      ],
      "source": [
        "# Inspect the generated plan\n",
        "if plan:\n",
        "    print(\"Generated Plan:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for i, step in enumerate(plan.steps):\n",
        "        print(f\"\\nStep {i + 1}: {step.task}\")\n",
        "        print(f\"  Player: {step.player}\")\n",
        "        print(f\"  Rationale: {step.rationale}\")\n",
        "        print(f\"  Inputs: {step.inputs}\")\n",
        "        print(f\"  Outputs: {step.outputs}\")\n",
        "else:\n",
        "    print(\"No plan to inspect. Run the plan generation cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "925d5be1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan as Dictionary:\n",
            "==================================================\n",
            "{'steps': [{'inputs': {},\n",
            "            'outputs': ['file_info'],\n",
            "            'player': 'data_analyst',\n",
            "            'rationale': 'First, gather basic file information such as size '\n",
            "                         'and estimated row count to understand the dataset '\n",
            "                         'scale.',\n",
            "            'task': 'get_file_info'},\n",
            "           {'inputs': {'file_path': 'file_info'},\n",
            "            'outputs': ['row_count'],\n",
            "            'player': 'data_analyst',\n",
            "            'rationale': 'Get the exact number of rows in the dataset to '\n",
            "                         'ensure we have a precise count for our metadata.',\n",
            "            'task': 'get_row_count'},\n",
            "           {'inputs': {'file_path': 'file_info'},\n",
            "            'outputs': ['column_names'],\n",
            "            'player': 'schema_expert',\n",
            "            'rationale': 'Identify the column names to understand the schema '\n",
            "                         'of the dataset.',\n",
            "            'task': 'get_column_names'},\n",
            "           {'inputs': {'file_path': 'file_info'},\n",
            "            'outputs': ['data_types'],\n",
            "            'player': 'schema_expert',\n",
            "            'rationale': 'Determine the data types of each column to provide '\n",
            "                         'accurate schema information.',\n",
            "            'task': 'get_data_types'},\n",
            "           {'inputs': {'file_path': 'file_info'},\n",
            "            'outputs': ['column_statistics'],\n",
            "            'player': 'data_analyst',\n",
            "            'rationale': 'Extract basic statistics for all columns to provide '\n",
            "                         \"detailed insights into the dataset's numerical \"\n",
            "                         'summaries and distributions.',\n",
            "            'task': 'get_column_statistics'},\n",
            "           {'inputs': {'file_path': 'file_info'},\n",
            "            'outputs': ['missing_values'],\n",
            "            'player': 'data_analyst',\n",
            "            'rationale': 'Assess the quality of the dataset by identifying the '\n",
            "                         'count of missing values in each column.',\n",
            "            'task': 'get_missing_values'},\n",
            "           {'inputs': {'column_names': 'column_names',\n",
            "                       'column_statistics': 'column_statistics',\n",
            "                       'data_types': 'data_types',\n",
            "                       'file_info': 'file_info',\n",
            "                       'missing_values': 'missing_values',\n",
            "                       'row_count': 'row_count'},\n",
            "            'outputs': ['final_metadata'],\n",
            "            'player': 'metadata_specialist',\n",
            "            'rationale': 'Combine all extracted information into the required '\n",
            "                         'metadata standard format.',\n",
            "            'task': 'format_final_metadata'}]}\n"
          ]
        }
      ],
      "source": [
        "# View plan as dictionary (full details)\n",
        "if plan:\n",
        "    print(\"Plan as Dictionary:\")\n",
        "    print(\"=\" * 50)\n",
        "    pprint(plan.model_dump())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72de2015",
      "metadata": {},
      "source": [
        "## 4. Plan Validation\n",
        "\n",
        "Validate the plan's dataflow dependencies to ensure all inputs are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2ef8eeb7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Plan dataflow is valid.\n"
          ]
        }
      ],
      "source": [
        "# Validate plan dataflow\n",
        "from src.orchestrator.utils import validate_plan_dataflow\n",
        "\n",
        "if plan:\n",
        "    # Convert to dict list for validation\n",
        "    plan_dicts = plan.to_dict_list()\n",
        "    \n",
        "    is_valid, message = validate_plan_dataflow(plan_dicts)\n",
        "    \n",
        "    if is_valid:\n",
        "        print(f\"✅ {message}\")\n",
        "    else:\n",
        "        print(f\"❌ {message}\")\n",
        "else:\n",
        "    print(\"No plan to validate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82813c1a",
      "metadata": {},
      "source": [
        "## 5. Prepare Dataset\n",
        "\n",
        "Ensure a test dataset exists for execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c59d2164",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset found at ../data/biota.csv\n",
            "\n",
            "Dataset Preview (391018 rows, 4 columns):\n",
            "   sample_id  sibes_id  abundance_m2  afdm_m2\n",
            "0      33941        16       28.8716   0.4331\n",
            "1      33941        20      115.4866   0.6986\n",
            "2      33941        21      115.4866   0.0000\n",
            "3      33941       289      230.9732   0.0462\n",
            "4      33942        20      288.7165   0.7391\n"
          ]
        }
      ],
      "source": [
        "# Check if dataset exists, create sample if not\n",
        "import pandas as pd\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    print(f\"Dataset not found at {DATASET_PATH}\")\n",
        "    print(\"Creating sample dataset...\")\n",
        "    \n",
        "    os.makedirs(os.path.dirname(DATASET_PATH), exist_ok=True)\n",
        "    \n",
        "    sample_df = pd.DataFrame({\n",
        "        \"id\": [1, 2, 3, 4, 5],\n",
        "        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n",
        "        \"age\": [25, 30, 35, 28, 32],\n",
        "        \"city\": [\"NYC\", \"LA\", \"Chicago\", \"NYC\", \"Boston\"],\n",
        "        \"salary\": [50000, 60000, 75000, 55000, 80000]\n",
        "    })\n",
        "    sample_df.to_csv(DATASET_PATH, index=False)\n",
        "    print(f\"✅ Sample dataset created at {DATASET_PATH}\")\n",
        "else:\n",
        "    print(f\"✅ Dataset found at {DATASET_PATH}\")\n",
        "\n",
        "# Preview the dataset\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"\\nDataset Preview ({len(df)} rows, {len(df.columns)} columns):\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91705e65",
      "metadata": {},
      "source": [
        "## 6. Full Execution\n",
        "\n",
        "Execute the complete pipeline: plan generation + parallel players + debate.\n",
        "\n",
        "**Note**: This will make multiple LLM calls and may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d8218b89",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-15 14:24:53,720 - INFO - ============================================================\n",
            "2026-01-15 14:24:53,721 - INFO - STARTING PLAN EXECUTION\n",
            "2026-01-15 14:24:53,721 - INFO - Dataset: ../data/biota.csv\n",
            "2026-01-15 14:24:53,721 - INFO - Steps: 7\n",
            "2026-01-15 14:24:53,723 - INFO - ============================================================\n",
            "2026-01-15 14:24:53,724 - INFO - \n",
            "2026-01-15 14:24:53,725 - INFO - ==================== STEP 1/7 ====================\n",
            "2026-01-15 14:24:53,726 - INFO - Task: get_file_info\n",
            "2026-01-15 14:24:53,726 - INFO - Player: data_analyst\n",
            "2026-01-15 14:24:53,727 - INFO - Rationale: First, gather basic file information such as size and estimated row count to understand the dataset scale.\n",
            "2026-01-15 14:24:53,730 - INFO - --- STEP 0: PARALLEL EXECUTION ---\n",
            "2026-01-15 14:24:53,731 - INFO - Task: get_file_info\n",
            "2026-01-15 14:24:53,733 - INFO - Players: 2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing plan with parallel players and debate...\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-15 14:27:05,157 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:27:05,160 - INFO -   Player 'data_analyst_1' completed execution\n",
            "2026-01-15 14:27:30,232 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:27:30,234 - INFO -   Player 'schema_expert_2' completed execution\n",
            "2026-01-15 14:27:30,235 - INFO - Max debate rounds (1) reached, synthesizing\n",
            "2026-01-15 14:27:30,237 - INFO - --- STEP 0: SYNTHESIS ---\n",
            "2026-01-15 14:27:35,672 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:27:35,675 - INFO -   Synthesis complete. Produced artifacts: ['file_info']\n",
            "2026-01-15 14:27:35,676 - INFO - Step 1 completed successfully\n",
            "2026-01-15 14:27:35,677 - INFO -   Artifacts produced: ['file_info']\n",
            "2026-01-15 14:27:35,678 - INFO - \n",
            "2026-01-15 14:27:35,679 - INFO - ==================== STEP 2/7 ====================\n",
            "2026-01-15 14:27:35,680 - INFO - Task: get_row_count\n",
            "2026-01-15 14:27:35,681 - INFO - Player: data_analyst\n",
            "2026-01-15 14:27:35,681 - INFO - Rationale: Get the exact number of rows in the dataset to ensure we have a precise count for our metadata.\n",
            "2026-01-15 14:27:35,683 - INFO - --- STEP 1: PARALLEL EXECUTION ---\n",
            "2026-01-15 14:27:35,684 - INFO - Task: get_row_count\n",
            "2026-01-15 14:27:35,684 - INFO - Players: 2\n",
            "2026-01-15 14:28:07,096 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:28:07,098 - INFO -   Player 'data_analyst_1' completed execution\n",
            "2026-01-15 14:28:25,238 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:28:25,240 - INFO -   Player 'schema_expert_2' completed execution\n",
            "2026-01-15 14:28:25,241 - INFO - Max debate rounds (1) reached, synthesizing\n",
            "2026-01-15 14:28:25,242 - INFO - --- STEP 1: SYNTHESIS ---\n",
            "2026-01-15 14:28:30,360 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:28:30,362 - INFO -   Synthesis complete. Produced artifacts: ['row_count']\n",
            "2026-01-15 14:28:30,364 - INFO - Step 2 completed successfully\n",
            "2026-01-15 14:28:30,365 - INFO -   Artifacts produced: ['row_count']\n",
            "2026-01-15 14:28:30,365 - INFO - \n",
            "2026-01-15 14:28:30,366 - INFO - ==================== STEP 3/7 ====================\n",
            "2026-01-15 14:28:30,366 - INFO - Task: get_column_names\n",
            "2026-01-15 14:28:30,366 - INFO - Player: schema_expert\n",
            "2026-01-15 14:28:30,367 - INFO - Rationale: Identify the column names to understand the schema of the dataset.\n",
            "2026-01-15 14:28:30,368 - INFO - --- STEP 2: PARALLEL EXECUTION ---\n",
            "2026-01-15 14:28:30,369 - INFO - Task: get_column_names\n",
            "2026-01-15 14:28:30,369 - INFO - Players: 2\n",
            "2026-01-15 14:28:47,765 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:28:47,768 - INFO -   Player 'data_analyst_1' completed execution\n",
            "2026-01-15 14:28:57,082 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:28:57,086 - INFO -   Player 'schema_expert_2' completed execution\n",
            "2026-01-15 14:28:57,087 - INFO - Max debate rounds (1) reached, synthesizing\n",
            "2026-01-15 14:28:57,088 - INFO - --- STEP 2: SYNTHESIS ---\n",
            "2026-01-15 14:28:58,237 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:28:58,240 - INFO -   Synthesis complete. Produced artifacts: ['column_names']\n",
            "2026-01-15 14:28:58,241 - INFO - Step 3 completed successfully\n",
            "2026-01-15 14:28:58,243 - INFO -   Artifacts produced: ['column_names']\n",
            "2026-01-15 14:28:58,243 - INFO - \n",
            "2026-01-15 14:28:58,244 - INFO - ==================== STEP 4/7 ====================\n",
            "2026-01-15 14:28:58,244 - INFO - Task: get_data_types\n",
            "2026-01-15 14:28:58,244 - INFO - Player: schema_expert\n",
            "2026-01-15 14:28:58,245 - INFO - Rationale: Determine the data types of each column to provide accurate schema information.\n",
            "2026-01-15 14:28:58,247 - INFO - --- STEP 3: PARALLEL EXECUTION ---\n",
            "2026-01-15 14:28:58,247 - INFO - Task: get_data_types\n",
            "2026-01-15 14:28:58,248 - INFO - Players: 2\n",
            "2026-01-15 14:29:23,195 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:29:23,197 - INFO -   Player 'data_analyst_1' completed execution\n",
            "2026-01-15 14:29:48,812 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:29:48,815 - INFO -   Player 'schema_expert_2' completed execution\n",
            "2026-01-15 14:29:48,816 - INFO - Max debate rounds (1) reached, synthesizing\n",
            "2026-01-15 14:29:48,818 - INFO - --- STEP 3: SYNTHESIS ---\n",
            "2026-01-15 14:29:53,109 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:29:53,111 - INFO -   Synthesis complete. Produced artifacts: ['data_types']\n",
            "2026-01-15 14:29:53,112 - INFO - Step 4 completed successfully\n",
            "2026-01-15 14:29:53,112 - INFO -   Artifacts produced: ['data_types']\n",
            "2026-01-15 14:29:53,113 - INFO - \n",
            "2026-01-15 14:29:53,113 - INFO - ==================== STEP 5/7 ====================\n",
            "2026-01-15 14:29:53,113 - INFO - Task: get_column_statistics\n",
            "2026-01-15 14:29:53,113 - INFO - Player: data_analyst\n",
            "2026-01-15 14:29:53,114 - INFO - Rationale: Extract basic statistics for all columns to provide detailed insights into the dataset's numerical summaries and distributions.\n",
            "2026-01-15 14:29:53,115 - INFO - --- STEP 4: PARALLEL EXECUTION ---\n",
            "2026-01-15 14:29:53,115 - INFO - Task: get_column_statistics\n",
            "2026-01-15 14:29:53,115 - INFO - Players: 2\n",
            "2026-01-15 14:30:23,874 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:30:23,876 - INFO -   Player 'data_analyst_1' completed execution\n",
            "2026-01-15 14:30:54,951 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:30:54,953 - INFO -   Player 'schema_expert_2' completed execution\n",
            "2026-01-15 14:30:54,953 - INFO - Max debate rounds (1) reached, synthesizing\n",
            "2026-01-15 14:30:54,954 - INFO - --- STEP 4: SYNTHESIS ---\n",
            "2026-01-15 14:31:10,583 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:31:10,585 - INFO -   Synthesis complete. Produced artifacts: ['column_statistics']\n",
            "2026-01-15 14:31:10,587 - INFO - Step 5 completed successfully\n",
            "2026-01-15 14:31:10,587 - INFO -   Artifacts produced: ['column_statistics']\n",
            "2026-01-15 14:31:10,587 - INFO - \n",
            "2026-01-15 14:31:10,588 - INFO - ==================== STEP 6/7 ====================\n",
            "2026-01-15 14:31:10,588 - INFO - Task: get_missing_values\n",
            "2026-01-15 14:31:10,589 - INFO - Player: data_analyst\n",
            "2026-01-15 14:31:10,590 - INFO - Rationale: Assess the quality of the dataset by identifying the count of missing values in each column.\n",
            "2026-01-15 14:31:10,592 - INFO - --- STEP 5: PARALLEL EXECUTION ---\n",
            "2026-01-15 14:31:10,593 - INFO - Task: get_missing_values\n",
            "2026-01-15 14:31:10,593 - INFO - Players: 2\n",
            "2026-01-15 14:31:21,491 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:31:21,493 - INFO -   Player 'data_analyst_1' completed execution\n",
            "2026-01-15 14:31:35,863 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:31:35,865 - INFO -   Player 'schema_expert_2' completed execution\n",
            "2026-01-15 14:31:35,866 - INFO - Max debate rounds (1) reached, synthesizing\n",
            "2026-01-15 14:31:35,868 - INFO - --- STEP 5: SYNTHESIS ---\n",
            "2026-01-15 14:31:39,076 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:31:39,078 - INFO -   Synthesis complete. Produced artifacts: ['missing_values']\n",
            "2026-01-15 14:31:39,080 - INFO - Step 6 completed successfully\n",
            "2026-01-15 14:31:39,081 - INFO -   Artifacts produced: ['missing_values']\n",
            "2026-01-15 14:31:39,081 - INFO - \n",
            "2026-01-15 14:31:39,082 - INFO - ==================== STEP 7/7 ====================\n",
            "2026-01-15 14:31:39,082 - INFO - Task: format_final_metadata\n",
            "2026-01-15 14:31:39,083 - INFO - Player: metadata_specialist\n",
            "2026-01-15 14:31:39,083 - INFO - Rationale: Combine all extracted information into the required metadata standard format.\n",
            "2026-01-15 14:31:39,084 - INFO - --- STEP 6: PARALLEL EXECUTION ---\n",
            "2026-01-15 14:31:39,085 - INFO - Task: format_final_metadata\n",
            "2026-01-15 14:31:39,085 - INFO - Players: 2\n",
            "2026-01-15 14:32:09,096 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:32:09,099 - INFO -   Player 'data_analyst_1' completed execution\n",
            "2026-01-15 14:32:51,751 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:32:51,754 - INFO -   Player 'schema_expert_2' completed execution\n",
            "2026-01-15 14:32:51,754 - INFO - Max debate rounds (1) reached, synthesizing\n",
            "2026-01-15 14:32:51,755 - INFO - --- STEP 6: SYNTHESIS ---\n",
            "2026-01-15 14:33:06,174 - INFO - HTTP Request: POST https://willma.surf.nl/api/v0/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2026-01-15 14:33:06,176 - INFO -   Synthesis complete. Produced artifacts: ['final_metadata']\n",
            "2026-01-15 14:33:06,177 - INFO - Step 7 completed successfully\n",
            "2026-01-15 14:33:06,177 - INFO -   Artifacts produced: ['final_metadata']\n",
            "2026-01-15 14:33:06,177 - INFO - \n",
            "2026-01-15 14:33:06,178 - INFO - ============================================================\n",
            "2026-01-15 14:33:06,178 - INFO - PLAN EXECUTION COMPLETE\n",
            "2026-01-15 14:33:06,178 - INFO - Steps completed: 7/7\n",
            "2026-01-15 14:33:06,178 - INFO - Overall success: True\n",
            "2026-01-15 14:33:06,179 - INFO - ============================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Execution completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Execute the plan with parallel players and debate\n",
        "# This runs the full pipeline\n",
        "\n",
        "if plan:\n",
        "    print(\"Executing plan with parallel players and debate...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    result = orchestrator.execute_plan(\n",
        "        plan=plan,\n",
        "        dataset_path=DATASET_PATH,\n",
        "        metadata_standard=metadata_standard\n",
        "    )\n",
        "    \n",
        "    if result.success:\n",
        "        print(\"\\n✅ Execution completed successfully!\")\n",
        "    else:\n",
        "        print(f\"\\n❌ Execution failed: {result.error}\")\n",
        "else:\n",
        "    print(\"No plan to execute. Run plan generation first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28440563",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect execution results\n",
        "if 'result' in dir() and result:\n",
        "    print(\"Execution Results Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Steps Completed: {result.steps_completed}/{result.plan_steps_count}\")\n",
        "    \n",
        "    print(\"\\n--- Step Results ---\")\n",
        "    for step_result in result.step_results:\n",
        "        print(f\"\\nStep {step_result.step_index + 1}: {step_result.task}\")\n",
        "        print(f\"  Player Role: {step_result.player_role}\")\n",
        "        print(f\"  Success: {step_result.success}\")\n",
        "        print(f\"  Debate Rounds: {step_result.debate_rounds_completed}\")\n",
        "        print(f\"  Artifacts Produced: {list(step_result.artifacts.keys())}\")\n",
        "        if step_result.error:\n",
        "            print(f\"  Error: {step_result.error}\")\n",
        "else:\n",
        "    print(\"No results to inspect. Run the execution cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d048cc6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View final workspace and metadata\n",
        "if 'result' in dir() and result:\n",
        "    print(\"Final Workspace Artifacts:\")\n",
        "    print(\"=\" * 50)\n",
        "    for name, value in result.final_workspace.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        # Truncate long values for display\n",
        "        value_str = str(value)\n",
        "        if len(value_str) > 500:\n",
        "            print(value_str[:500] + \"...\")\n",
        "        else:\n",
        "            print(value_str)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Final Metadata:\")\n",
        "    print(\"=\" * 50)\n",
        "    pprint(result.final_metadata)\n",
        "else:\n",
        "    print(\"No results to display.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b105ff1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for key, value in result.final_metadata['artifacts'].items():\n",
        "    print(f\"\\n--- {key} ---\")\n",
        "    print(value)\n",
        "    print(\"\\n\" + \"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c4d77f",
      "metadata": {},
      "source": [
        "## 7. Test Tools Directly\n",
        "\n",
        "Test the available tools on the dataset without going through the full pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fc07c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tools directly on the dataset\n",
        "from src.tools import pandas_tools\n",
        "\n",
        "print(\"Testing Tools on Dataset:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get row count\n",
        "print(\"\\n1. Row Count:\")\n",
        "row_count = pandas_tools.get_row_count.invoke({\"file_path\": DATASET_PATH})\n",
        "print(f\"   {row_count} rows\")\n",
        "\n",
        "# Get column names\n",
        "print(\"\\n2. Column Names:\")\n",
        "columns = pandas_tools.get_column_names.invoke({\"file_path\": DATASET_PATH})\n",
        "print(f\"   {columns}\")\n",
        "\n",
        "# Get data types\n",
        "print(\"\\n3. Data Types:\")\n",
        "dtypes = pandas_tools.get_data_types.invoke({\"file_path\": DATASET_PATH})\n",
        "for col, dtype in dtypes.items():\n",
        "    print(f\"   {col}: {dtype}\")\n",
        "\n",
        "# Get missing values\n",
        "print(\"\\n4. Missing Values:\")\n",
        "missing = pandas_tools.get_missing_values.invoke({\"file_path\": DATASET_PATH})\n",
        "for col, count in missing.items():\n",
        "    print(f\"   {col}: {count}\")\n",
        "\n",
        "# Get sample rows\n",
        "print(\"\\n5. Sample Rows:\")\n",
        "sample = pandas_tools.get_sample_rows.invoke({\"file_path\": DATASET_PATH})\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea73bdfe",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the full metadata extraction pipeline:\n",
        "\n",
        "1. **Configuration** - Set file type, metadata standard, and execution topology\n",
        "2. **Exploration** - Viewed available topologies and player roles\n",
        "3. **Plan Generation** - LLM generated a step-by-step extraction plan\n",
        "4. **Validation** - Verified plan dataflow dependencies\n",
        "5. **Dataset Preparation** - Ensured test dataset exists\n",
        "6. **Full Execution** - Ran parallel players with debate on each step\n",
        "7. **Tools Testing** - Tested individual tools on the dataset\n",
        "\n",
        "### Key Components:\n",
        "- **Orchestrator**: Coordinates planning and execution\n",
        "- **Players**: Execute tasks and participate in debates  \n",
        "- **Tools**: Extract actual data from datasets\n",
        "- **Topology**: Configures parallelism and debate rounds\n",
        "\n",
        "### Configuration Options:\n",
        "- Edit `src/config.py` to change the LLM model\n",
        "- Modify `TOPOLOGY_NAME` to change execution strategy\n",
        "- Add new player roles in `src/players/configs.py`\n",
        "- Add new tools in `src/tools/pandas_tools.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89e10eb",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sam2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
